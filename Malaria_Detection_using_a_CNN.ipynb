{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First we need to import the dependencies like ***numpy***, ***tensorflow***, ***matplotlib***, etc.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AAN8D0aFzaX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB7Jtxwjyd8F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.regularizers import L2\n",
        "from keras.layers import Dense, Input, Conv2D, MaxPool2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then using the tensorflow_datasets, we can import the malaria dataset directly from thier servers."
      ],
      "metadata": {
        "id": "qzfAxDk7z4dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, dataset_info = tfds.load('malaria', with_info = True, as_supervised= True,split = ['train'])"
      ],
      "metadata": {
        "id": "SOrWi3FHyyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to divide the dataset into training, cross-validation and testing sets for later use. We can adjust the split ratio of each of these sub-sets."
      ],
      "metadata": {
        "id": "NiGaYKvp0Mrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_dataset(dataset):\n",
        "  train_split = 0.8\n",
        "  val_split = 0.1\n",
        "  test_split = 0.1\n",
        "  size = len(dataset)\n",
        "  train_dataset = dataset.take(int(train_split * size))\n",
        "  temp = dataset.skip(int(train_split * size))\n",
        "  validation_dataset = temp.take(int(val_split * size))\n",
        "  test_dataset = temp.skip(int(val_split * size))\n",
        "  return train_dataset, validation_dataset, test_dataset\n",
        "\n",
        "def preprocessing(image, label):\n",
        "  image = preprocess_input(image)\n",
        "  image = tf.image.resize(image, (224,224))\n",
        "  return image, label\n",
        "train_dataset, val_dataset, test_dataset = divide_dataset(dataset[0])"
      ],
      "metadata": {
        "id": "9r6UvuvJy3JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocessing)\n",
        "val_dataset = val_dataset.map(preprocessing)\n",
        "test_dataset = test_dataset.map(preprocessing)"
      ],
      "metadata": {
        "id": "nhR_GTf9y8S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For better learning, we make the training dataset to shuffle itself after each iteration in order to avoid overfitting. We set the batch size to 12."
      ],
      "metadata": {
        "id": "bzwAP_oH0kRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_Dataset = train_dataset.shuffle(buffer_size = 100, reshuffle_each_iteration= True).batch(12).prefetch(tf.data.AUTOTUNE)\n",
        "val_Dataset = val_dataset.shuffle(buffer_size = 100, reshuffle_each_iteration= True).batch(12).prefetch(tf.data.AUTOTUNE)\n",
        "test_Dataset = test_dataset.batch(1)"
      ],
      "metadata": {
        "id": "vGGFZZsJy_Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize some of the training example, we can use matplotlib to make subplots of these images."
      ],
      "metadata": {
        "id": "fLuWWAXD1C-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "for i, (image, label) in enumerate(train_dataset.take(16)):\n",
        "  plt.subplot(4,4,i+1)\n",
        "  if i == 0:\n",
        "    print(image)\n",
        "  plt.imshow(image)\n",
        "  plt.title(label.numpy())\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkd61-hBzE-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our model, we will be using a single *Convolutional* Layer with kernel of shape (5,5) and 32 units, two *Dense* Layers with *Regularization* (lambda_ = 0.01) and tow *Dropout* Layers just to avoid overfitting. Lastly, we used sigmoid activation as its a **Binary Classification** problem."
      ],
      "metadata": {
        "id": "f6mgGPD71tDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    Input(shape = (224,224,3)),\n",
        "    Conv2D(32, (5,5), activation = 'relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(1028, activation = 'relu', kernel_regularizer = L2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1028, activation = 'relu', kernel_regularizer = L2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model.compile(loss = BinaryCrossentropy(), optimizer = Adam(), metrics = 'accuracy')"
      ],
      "metadata": {
        "id": "6ulkTOouzJRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "About training for 10 Epochs, both the training and validation accuracy was above 89%. Which indicates no overfitting neither underfitting. The model was converging."
      ],
      "metadata": {
        "id": "Kc7RMUld14sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_Dataset, validation_data = val_Dataset, epochs = 10)"
      ],
      "metadata": {
        "id": "w-5X3lfOzL0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_Dataset)"
      ],
      "metadata": {
        "id": "8MWRXudLzSG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can make predictions on the data, which was never seen by the model before."
      ],
      "metadata": {
        "id": "8j-ymT9T2XvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_Dataset)"
      ],
      "metadata": {
        "id": "xAgexVbTzVL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}